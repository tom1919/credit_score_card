---
title: "Untitled"
output: html_document
---


```{r}
all2
```


one hot encode
```{r}
enconder <- onehot(all2, max_levels = 15)

all3 <- predict(enconder, all2) %>% 
  as.data.frame() %>%
  select(-`GB=0`)

names(all3) <- make.names(names(all3))

names(all3)[1] = "GB"
all3 <- all3 %>% mutate(GB = ifelse(GB == 1, "B", "G"))
all3$GB <- as.factor(all3$GB)


```



Split the all2 df into accepts and rejects. Split the accepts into train and test
```{r}
accepts2 <- all3 %>% filter(!is.na(GB))
reject2 <- all3 %>% filter(is.na(GB))

set.seed(888)
train <- accepts2 %>% sample_frac(size = .7) 
test <- accepts2 %>% anti_join(train, by = "id")
```

Explore variable importance with Boruta algorithm
```{r}
set.seed(888)
# run the algorithm
boruta_train <- Boruta(GB ~. , data = train %>% select(-id , -`X_freq_`), 
                       doTrace = 2, maxRuns = 40)
# make a rough cut for any variables that the algo wasn't able to decide on
final_boruta <- TentativeRoughFix(boruta_train)

# create a df of feature importance
boruta_df <- attStats(final_boruta) %>% 
  rownames_to_column("col") %>%
  arrange(desc(meanImp))

# look at the variables that were confirmed as important
boruta_df %>% filter(decision == "Confirmed")
```

```{r}
imp_feat <- boruta_df %>% filter(decision == "Confirmed") %>% pull(col)
train2 <- train %>% select(c("GB", imp_feat))
```


```{r}
#  create list with indexes for each cv fold
#  this makes the models more comaparable bc each one will use the same folds
set.seed(888)
cv_folds <- createFolds(train$GB, k = 5, list = TRUE, returnTrain = TRUE)

# create object to control model tuning
# save predictions to get out of sample predictions for stacking
ctrl <- trainControl(method = "cv", index = cv_folds, savePredictions = 'final',
                     verboseIter = T, classProbs = TRUE, 
                     summaryFunction=twoClassSummary)
```

```{r}
set.seed(888)
enet_grid <- expand.grid(alpha = seq(0,1, length = 15), 
                         lambda = c(seq(0.005, 1, length =  300))) %>%
  sample_frac(.4) 

enet <- train(GB ~. , data = train %>% select(-id , -`X_freq_`), 
              tuneGrid = enet_grid,
              preProcess = c("center", "scale"), metric = "ROC",
              method = "glmnet", trControl = ctrl)

enet$results %>% arrange(desc(ROC)) #0.7390735

# get the coeficents for best fit lridge model
enet_coef <- coef(enet$finalModel, enet$bestTune$lambda) %>% 
  as.matrix() %>% as.data.frame() %>% rownames_to_column("feat")

names(enet_coef) <- c("feat", "coef")

# arrange coef by largest absolute value (variable importance)
enet_coef %>% arrange(desc(abs(coef)))

#enet$results %>% arrange(desc(Accuracy))
```


```{r}
set.seed(888)
enet2_grid <- expand.grid(alpha = seq(0,1, length = 15), 
                         lambda = c(seq(0.005, 1, length =  300))) %>%
  sample_frac(.4) 

enet2 <- train(GB ~. , data = train2, 
              tuneGrid = enet2_grid,
              preProcess = c("center", "scale"), metric = "ROC",
              method = "glmnet", trControl = ctrl)

enet2$results %>% arrange(desc(ROC)) #0.7390735

# get the coeficents for best fit lridge model
enet2_coef <- coef(enet2$finalModel, enet2$bestTune$lambda) %>% 
  as.matrix() %>% as.data.frame() %>% rownames_to_column("feat")

names(enet2_coef) <- c("feat", "coef")

# arrange coef by largest absolute value (variable importance)
enet2_coef %>% arrange(desc(abs(coef)))

#enet$results %>% arrange(desc(Accuracy))
```


```{r}
# RandomForest
set.seed(888)
rf2_grid <- expand.grid(mtry = c(2,5,9,12,15, 18, 25), 
                       min.node.size = c( 3, 5, 9, 15, 18, 23, 28),
                       splitrule = c( 'extratrees'))

rf2 <- train(GB ~. , data = train2,
            tuneGrid = rf2_grid, num.tree = 2000, 
            importance = 'permutation', metric = "ROC", method = "ranger", 
            trControl = ctrl)

rf2$results %>% arrange(desc(ROC)) #0.7355033
varImp(rf2)
```

```{r}
# RandomForest
set.seed(888)
rf3_grid <- expand.grid(mtry = c(2,5,9,12,15, 18,21, 25, 30), 
                       min.node.size = c( 3, 5, 9, 15, 18, 23, 28, 32, 37,45),
                       splitrule = c( 'extratrees'))

rf3 <- train(GB ~. , data = train2,
            tuneGrid = rf3_grid, num.tree = 2000, 
            importance = 'permutation', metric = "ROC", method = "ranger", 
            trControl = ctrl)

rf3$results %>% arrange(desc(ROC)) # mtry=12, min.node=45, roc = .7492746

varImp(rf3)
```