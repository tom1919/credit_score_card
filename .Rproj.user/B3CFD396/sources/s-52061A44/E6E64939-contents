---
title: "R Notebook"
output: html_notebook
---

The purpose of this notebook is to explore the data and create features

Load custom functions, libraries and data
```{r}
source("../scripts/helper_functions.R")
LoadPackages(c("dplyr", 'haven', 'missForest', "stringr",
               "Boruta", "tibble", 'caret', "onehot"))


accepts <- read_sas("../data/raw/accepted_customers.sas7bdat")
rejects <- read_sas("../data/raw/rejected_customers.sas7bdat")

accepts[accepts==""] <- NA
rejects[rejects==""] <- NA
```

Summary of the accepts data set
```{r}
summarize_df(accepts)
```

Summary of the rejects data set
```{r}
summarize_df(rejects)
```

Subset rejects cols to only the ones in accepts df
Bind accepts df with rejects df
```{r}
not_in_accepts <- setdiff(names(rejects), names(accepts))
rejects <- rejects %>% select(setdiff( names(rejects), not_in_accepts))

all <- bind_rows(accepts, rejects)
```

Issues with the data
```{r}
# 123 NA values for time at address
all %>% filter(TMADD == 999) %>% nrow()
# 45 NA values for time at job
all %>% filter(TMJOB1 == 999) %>% nrow()
# Someone has 23 childern
table(all$CHILDREN)
# Someone has 25 in their household
table(all$PERS_H)
```

Replace implausible values with NA and cast non numeric variables to factors
```{r}
all <- all %>%
  mutate(TMADD = ifelse(TMADD == 999, NA, TMADD),
         TMJOB1 = ifelse(TMJOB1 == 999, NA, TMJOB1),
         CHILDREN = ifelse(CHILDREN == 23, NA, CHILDREN),
         PERS_H = ifelse(PERS_H == 25, NA, PERS_H))

factor_cols <- c('TEL', 'STATUS', 'BUREAU', 'LOCATION', 'REGN', 'DIV', 'CARDS', 
                 'PRODUCT', 'RESID', 'NAT', 'PROF', 'CAR', 'GB', 'TITLE', 
                 'FINLOAN','EC_CARD')

all <- all %>% mutate_at(factor_cols, funs(as.factor(.)))
```
Create dummy vars that indicate variable was missing
```{r}
missing_flags <- all %>% 
  mutate(TMADD_unk = ifelse(is.na(TMADD), 1, 0),
         TMJOB1_unk = ifelse(is.na(TMJOB1), 1, 0),
         RESID_unk = ifelse(is.na(RESID), 1, 0)) %>%
  select(TMADD_unk, TMJOB1_unk, RESID_unk)
# not creating flag for product, children and pers_h because there was little 
# missing
```




Impute missing values
```{r}
impute_df <- all %>% select(-GB, -`_freq_`) # don't use target for imputation
target <- all %>% select(GB, `_freq_`)

set.seed(888)
mf_imp <- missForest(as.data.frame(impute_df), verbose = T)

all2 <- bind_cols(target, mf_imp$ximp) 
all2 <- bind_cols(all2, missing_flags)
```

Create features
```{r}
all2 <- all2 %>% 
  mutate(# round children and pers_h to nearest integer (imputation fractions)
         CHILDREN = round(CHILDREN),
         PERS_H = round(PERS_H),
         adult = PERS_H - CHILDREN,
         cash_per_pers = CASH / PERS_H,
         cash_per_adult = CASH / adult,
         income_per_pers = INCOME / PERS_H,
         income_per_adult = INCOME/ adult,
         # if cash is zero then division by zero so just impute 1.5 (avg)
         income_cash_ratio = ifelse(CASH == 0, 1.5, INCOME / CASH),
         total_loans = LOANS + NMBLOAN,
         id = row_number()) %>%
  select(GB, `_freq_`, id, everything())

# treat these vars as factors because they have 3 or less unq values
all2 <- all2 %>% 
  mutate_at(c('NMBLOAN', 'RESID', 'adult', 'TMADD_unk', 'TMJOB1_unk', 
              'RESID_unk'), funs(as.factor(.)))

summarize_df(all2) 
```

one hot encode
```{r}
enconder <- onehot(all2, max_levels = 15)

all3 <- predict(enconder, all2) %>% 
  as.data.frame() %>%
  select(-`GB=0`)

names(all3) <- make.names(names(all3))

names(all3)[1] = "GB"
all3 <- all3 %>% mutate(GB = ifelse(GB == 1, "B", "G"))
all3$GB <- as.factor(all3$GB)


```



Split the all2 df into accepts and rejects. Split the accepts into train and test
```{r}
accepts2 <- all3 %>% filter(!is.na(GB))
reject2 <- all3 %>% filter(is.na(GB))

set.seed(888)
train <- accepts2 %>% sample_frac(size = .7) 
test <- accepts2 %>% anti_join(train, by = "id")
```

Explore variable importance with Boruta algorithm
```{r}
set.seed(888)
# run the algorithm
boruta_train <- Boruta(GB ~. , data = train %>% select(-id , -`X_freq_`), 
                       doTrace = 2, maxRuns = 40)
# make a rough cut for any variables that the algo wasn't able to decide on
final_boruta <- TentativeRoughFix(boruta_train)

# create a df of feature importance
boruta_df <- attStats(final_boruta) %>% 
  rownames_to_column("col") %>%
  arrange(desc(meanImp))

# look at the variables that were confirmed as important
boruta_df %>% filter(decision == "Confirmed")
```

```{r}
imp_feat <- boruta_df %>% filter(decision == "Confirmed") %>% pull(col)
train2 <- train %>% select(c("GB", imp_feat))
```


```{r}
#  create list with indexes for each cv fold
#  this makes the models more comaparable bc each one will use the same folds
set.seed(888)
cv_folds <- createFolds(train$GB, k = 5, list = TRUE, returnTrain = TRUE)

# create object to control model tuning
# save predictions to get out of sample predictions for stacking
ctrl <- trainControl(method = "cv", index = cv_folds, savePredictions = 'final',
                     verboseIter = T, classProbs = TRUE, 
                     summaryFunction=twoClassSummary)
```

```{r}
set.seed(888)
enet_grid <- expand.grid(alpha = seq(0,1, length = 15), 
                         lambda = c(seq(0.005, 1, length =  300))) %>%
  sample_frac(.4) 

enet <- train(GB ~. , data = train %>% select(-id , -`X_freq_`), 
              tuneGrid = enet_grid,
              preProcess = c("center", "scale"), metric = "ROC",
              method = "glmnet", trControl = ctrl)

enet$results %>% arrange(desc(ROC)) #0.7390735

# get the coeficents for best fit lridge model
enet_coef <- coef(enet$finalModel, enet$bestTune$lambda) %>% 
  as.matrix() %>% as.data.frame() %>% rownames_to_column("feat")

names(enet_coef) <- c("feat", "coef")

# arrange coef by largest absolute value (variable importance)
enet_coef %>% arrange(desc(abs(coef)))

#enet$results %>% arrange(desc(Accuracy))
```


```{r}
set.seed(888)
enet2_grid <- expand.grid(alpha = seq(0,1, length = 15), 
                         lambda = c(seq(0.005, 1, length =  300))) %>%
  sample_frac(.4) 

enet2 <- train(GB ~. , data = train2, 
              tuneGrid = enet2_grid,
              preProcess = c("center", "scale"), metric = "ROC",
              method = "glmnet", trControl = ctrl)

enet2$results %>% arrange(desc(ROC)) #0.7390735

# get the coeficents for best fit lridge model
enet2_coef <- coef(enet2$finalModel, enet2$bestTune$lambda) %>% 
  as.matrix() %>% as.data.frame() %>% rownames_to_column("feat")

names(enet2_coef) <- c("feat", "coef")

# arrange coef by largest absolute value (variable importance)
enet2_coef %>% arrange(desc(abs(coef)))

#enet$results %>% arrange(desc(Accuracy))
```


```{r}
# RandomForest
set.seed(888)
rf2_grid <- expand.grid(mtry = c(2,5,9,12,15, 18, 25), 
                       min.node.size = c( 3, 5, 9, 15, 18, 23, 28),
                       splitrule = c( 'extratrees'))

rf2 <- train(GB ~. , data = train2,
            tuneGrid = rf2_grid, num.tree = 2000, 
            importance = 'permutation', metric = "ROC", method = "ranger", 
            trControl = ctrl)

rf2$results %>% arrange(desc(ROC)) #0.7355033
varImp(rf2)
```

```{r}
# RandomForest
set.seed(888)
rf3_grid <- expand.grid(mtry = c(2,5,9,12,15, 18,21, 25, 30), 
                       min.node.size = c( 3, 5, 9, 15, 18, 23, 28, 32, 37,45),
                       splitrule = c( 'extratrees'))

rf3 <- train(GB ~. , data = train2,
            tuneGrid = rf3_grid, num.tree = 2000, 
            importance = 'permutation', metric = "ROC", method = "ranger", 
            trControl = ctrl)

rf3$results %>% arrange(desc(ROC)) #0.7355033

varImp(rf3)
```