---
title: "Homework 2"
output: html_notebook
---

Custom fuctions
```{r}
LoadPackages <- function(packages) {
  # Load or install packages if they aren't already loaded.
  #
  # Args:
  #   packages: a vector of package names
  #
  for (package in packages) {
    if (!require(package, character.only=T, quietly=T)) {
      if (!package %in% installed.packages()) install.packages(package)
      library(package, character.only=T)
    }
  }
}

summarize_df <- function(df, r = 4) {
  # Create summary information about a data set
  #
  # Args:
  #   df = data frame 
  #   r = number of decimal places to round
  #
  # Return: A data frame containing various summary info about each column
  require(dplyr)
  summary_names <- c("col_name", 
                     "type", 
                     "num_unq",
                     "mode", 
                     "mode_ratio",                     
                     "num_missing", 
                     "num_na", 
                     "num_inf", 
                     "num_nan",
                     "min", 
                     "q1", 
                     "median", 
                     "mean", 
                     "q3", 
                     "max", 
                     "std_dev")
  col_summary <- data.frame(matrix(ncol = length(summary_names), nrow = ncol(df)))
  names(col_summary) <- summary_names
  
  for(i in 1:ncol(df)) {
    col <- df[,i]
    not_inf <- df[!is.infinite(col), i]
    freq_table <- sort(table(not_inf), decreasing=TRUE)[1]
    
    col_name <- names(df)[i]
    type <- class(col)
    num_inf <- length(df[is.infinite(col),i])
    num_nan <- length(df[is.nan(col),i])
    num_na <- length(df[is.na(col),i]) - num_nan
    num_missing <- num_na + num_inf + num_nan
    num_unq <- length(unique(not_inf[!is.na(not_inf)])) # NAs and INF values not included
    mode <- names(freq_table) # NAs and INF values not included
    mode_ratio <- unname(freq_table) / (length(col) - num_missing)
    
    
    if(is.numeric(col) == TRUE) {
      min <- min(col, na.rm = TRUE)
      q1 <- quantile(not_inf, .25, na.rm = TRUE) %>% unname()
      median <- median(not_inf, na.rm = TRUE)
      mean <- mean(not_inf, na.rm = TRUE)
      q3 <- quantile(not_inf, .75, na.rm = TRUE) %>% unname()
      max <- max(not_inf, na.rm = TRUE)
      std_dev <- sd(not_inf, na.rm = TRUE)
    } else {
      min <- NA
      q1 <- NA
      median <- NA
      mean <- NA
      q3 <- NA
      max <- NA
      std_dev <- NA
    }
    
    col_summary[i,] <- c(col_name, 
                         type, 
                         num_unq,
                         mode,
                         mode_ratio,
                         num_missing, 
                         num_na,
                         num_inf, 
                         num_nan,
                         min, 
                         q1, 
                         median, 
                         mean, 
                         q3, 
                         max, 
                         std_dev)
  }
  numerics <- dplyr::setdiff(names(col_summary),c("col_name", "type", "mode")) 
  col_summary <- dplyr::mutate_at(col_summary, numerics, funs(as.double(.)))
  col_summary <- dplyr::mutate_at(col_summary, numerics, funs(round(., r)))
  
  return(col_summary)
}

draw_confusion_matrix <- function(cm) {

  layout(matrix(c(1,1,2)))
  par(mar=c(2,2,2,2))
  plot(c(100, 345), c(300, 450), type = "n", xlab="", ylab="", xaxt='n', yaxt='n')
  title('CLASSIFICATION TABLE', cex.main=2)

  # create the matrix 
  rect(150, 430, 240, 370, col='#3F97D0')
  text(195, 440, 'No Purchase', cex=1.4, font=1)
  rect(250, 430, 340, 370, col='#F7AD50')
  text(295, 440, 'Purchase', cex=1.4, font=1)
  text(125, 370, 'Predicted', cex=1.8, srt=90, font = 2)
  text(245, 450, 'Actual', cex=1.8, font = 2)
  rect(150, 305, 240, 365, col='#F7AD50')
  rect(250, 305, 340, 365, col='#3F97D0')
  text(140, 400, 'No Purchase', cex=1.4, srt=90, font=1)
  text(140, 335, 'Purchase', cex=1.4, srt=90, font=1)

  # add in the cm results 
  res <- as.numeric(cm$table)
  text(195, 400, res[1], cex=1.9, font=2, col='white')
  text(195, 335, res[2], cex=1.9, font=2, col='white')
  text(295, 400, res[3], cex=1.9, font=2, col='white')
  text(295, 335, res[4], cex=1.9, font=2, col='white')

  # add in the specifics 
  plot(c(100, 0), c(100, 0), type = "n", xlab="", ylab="", main = 
         "ASSOCIATED STATISTICS", xaxt='n', yaxt='n', cex=1.8, font = 2)
  text(10, 85, names(cm$byClass[1]), cex=1.2, font=3)
  text(10, 70, round(as.numeric(cm$byClass[1]), 3), cex=1.2)
  text(30, 85, names(cm$byClass[2]), cex=1.2, font=3)
  text(30, 70, round(as.numeric(cm$byClass[2]), 3), cex=1.2)
  text(50, 87, names(cm$byClass[5]), cex=1.2, font=3)
  text(50, 71, round(as.numeric(cm$byClass[5]), 3), cex=1.2)
  text(70, 87, names(cm$byClass[6]), cex=1.2, font=3)
  text(70, 71, round(as.numeric(cm$byClass[6]), 3), cex=1.2)
  text(90, 87, names(cm$byClass[7]), cex=1.2, font=3)
  text(90, 71, round(as.numeric(cm$byClass[7]), 3), cex=1.2)
  
  # add in the accuracy information 
  text(30, 35, names(cm$overall[1]), cex=1.5, font=3)
  text(30, 19, round(as.numeric(cm$overall[1]), 3), cex=1.4)
  text(70, 35, names(cm$overall[2]), cex=1.5, font=3)
  text(70, 19, round(as.numeric(cm$overall[2]), 3), cex=1.4)
}  
```

Install and or load packages
```{r}
LoadPackages(c("haven", "dplyr", "Boruta", "tibble", "purrr", "ROCR",
               "stringr", 'brglm', 'tidyr', "olsrr", "caret", "car",
               "MASS", "missForest", "mgcv", "MLmetrics", "reshape2"))

select <- dplyr::select
```

Variables for file directories 
```{r}
data <- paste0('C:/Users/tommy/Google Drive/Coursework/',
               'logistic_regression/data/')
raw <- paste0('C:/Users/tommy/Google Drive/Coursework/',
               'logistic_regression/data/raw/')
```

Read in data
```{r}
insurance_t <- read_sas(paste0(raw,"insurance_t.sas7bdat")) 
```

Create data dictionary
```{r}
n <- ncol(insurance_t)
labels <- map_chr(1:n, function(x) attr(insurance_t[[x]], "label") )
col_names <- names(insurance_t)
data_dict <- data.frame(col_names, labels)
```

Summary of DF
```{r}
insurance_t <- as.data.frame(insurance_t)
df_info <- summarize_df(insurance_t) %>% 
  inner_join(data_dict, by = c("col_name" = "col_names")) %>%
  select(col_name, labels, everything())
# write.csv(df_info, paste0(data,"df_summary.csv"))
```

Cast binary columns as characters
```{r}
df_cast <- insurance_t

# vector of column names with only 2 unique values
binary_cols <- summarize_df(df_cast ,4) %>% 
  filter(num_unq == 2) %>% 
  pull(col_name)

# make sure there is only 0s and 1s
summary(df_cast [, binary_cols])

# look at variables to see if they logically should be factors
data_dict %>% 
  filter(col_names %in% binary_cols)

# cast binary cols as factors
df_cast  <- df_cast  %>%
  mutate_at(binary_cols, funs(as.character(.)))
```

Replace values in binary variables with 1 = Yes and 0 = No and cast all
character variables as factors
```{r}
# check if other variables should be factors
summarize_df(df_cast) %>% arrange(type)

cat_cols <- c("BRANCH", "RES", binary_cols)

# Cast 2 other categorical variables as factors
df_cast <- df_cast %>% 
  mutate_at(cat_cols, funs(as.character(.))) %>%
  mutate_at(binary_cols, funs(str_replace_all(., c("1" = "Yes", "0" = "No")))) %>%
  mutate_at(cat_cols, funs(as.factor(.)))
```


Create binary variables for if certain varibles are missing for the obs
# Informative missingness, censored missingness, structurlly missing pg. 41 of APM
```{r}
df_mvar <- df_cast

df_mvar <- df_mvar %>%  
  mutate(m1537 = ifelse(is.na(INCOME) & 
                          is.na(LORES) &
                          is.na(HMVAL), "Yes", "No"),
         m1075 = ifelse(is.na(PHONE) & 
                          is.na(POS) & 
                          is.na(POSAMT) & 
                          is.na(INV) & 
                          is.na(INVBAL) & 
                          is.na(CC) & 
                          is.na(CCBAL) & 
                          is.na(CCPURC), "Yes", "No")) %>%
  mutate_at(c("m1537", "m1075"), funs(as.factor(.)))
```

missForest is the most accurate but takes to long to run.
Using different imputation method
```{r}
# note: for some reason passing df to missForest gave error and I had to 
# convert to matrix and then convert back to df to make it work.  ¯\_(ツ)_/¯

# x_var <- train1 %>% select(-INS) %>%
#   as.matrix() %>% as.data.frame() 
# 
# num_cols <- setdiff(names(x_var), c(cat_cols, 'm1537', 'm1075'))
# 
# x_var <- x_var %>% mutate_at(num_cols, funs(as.numeric(.)))
# 
# 
# 
# set.seed(888)
# x_impd <- missForest(xmis = x_var, variablewise = TRUE, 
#                      verbose = TRUE)
# 
# ximp <- x_impd$ximp
# 
# train2 <- bind_cols(train1 %>% select(INS) , ximp)
# 
# saveRDS(x_impd, paste0(data,'x_impd.rds'))
```

Impute missing values using KNN
```{r}
# this method can't impute categorical data
# numerical variables are centered and scaled to make knn work
proc <-  preProcess(df_mvar, method = c('knnImpute'))

df_full <- predict(proc, df_mvar) %>% 
  select(-HMOWN, -INV, -CC)
```

Create some new predictors
```{r}
df_var <- df_full %>% 
  mutate_at(c("CD", "IRA", "MM", "LOC", "ILS", "MTG"), funs(as.numeric(.))) %>%
  mutate(INVT_ACCT_AMT = CD + IRA + MM -3,
         INVT_ACCT_BAL = CDBAL + IRABAL + MMBAL,
         LN_ACCT_AMT = LOC + ILS + MTG - 3,
         LN_ACCT_BAL = LOCBAL + ILSBAL + MTGBAL) %>%
  select(-CD, -IRA, -MM, -CDBAL, -IRABAL, -MMBAL, -LOC,
         -ILS, -MTG, -LOCBAL, -ILSBAL, -MTGBAL)
```

Create low volume branch predictor
```{r}
branch_ins_sold <- df_var %>% 
  filter(INS == "Yes") %>%
  group_by(BRANCH) %>%
  summarise(num_sold = n())

branch_ins_unsold <- df_var %>% 
  filter(INS == "No") %>%
  group_by(BRANCH) %>%
  summarise(num_unsold = n())

branch <- branch_ins_sold %>%
  full_join(branch_ins_unsold, by = "BRANCH") %>%
  mutate(per_sold = num_sold / (num_sold + num_unsold)) %>%
  arrange(per_sold)

# ggplot(branch, aes(x = per_sold)) + geom_histogram()

bad_branch <- branch %>% filter(per_sold <= .34) %>% pull(BRANCH)

df_var <- df_var %>%
  mutate(LOW_VOL_BRANCH = ifelse(BRANCH %in% bad_branch, "Yes", "No")) %>% 
  mutate_at('LOW_VOL_BRANCH', funs(as.factor(.))) %>%
  select(-BRANCH) 
```

Feature importance using boruta algo
```{r}
set.seed(888)
boruta_train <- Boruta(INS ~., data = df_var, doTrace = 2, maxRuns = 20)
final_boruta <- TentativeRoughFix(boruta_train)

boruta_df <- attStats(final_boruta) %>% 
  rownames_to_column("col") %>%
  arrange(desc(meanImp))

first6 <- boruta_df[1:6,] %>%
  pull(col)

train1 <- df_var[ ,c("INS", first6)]
```


Check for separation
```{r}
glm_fit <- glm(INS ~., data = train1, family = binomial(link = "logit"))
summary(glm_fit)

brglm_fit <- brglm(INS ~., data = train1, family = binomial(link = "logit"))

# some of the ratios diverage so separation is  an issue
separation.detection(glm_fit)
```

VIF 
```{r}
# all VIF is less than 2
mod_vif <- lm(INS ~ ., data = train1)
ols_vif_tol(mod_vif) %>% arrange(VIF)
```

Create squared terms
```{r}
# create new cols that are the orginal predictors squared
num_cols <- summarize_df(train1) %>% 
  filter(type == "numeric") %>% 
  pull(col_name)

sq_cols <- train1[, num_cols]

for (col in names(sq_cols)) {
  sq_cols[,col] <- sq_cols[,col] * sq_cols[,col]
}

sq_names <- names(sq_cols) %>% paste0("_sq")

names(sq_cols) <- sq_names

# Create dfs for testing squared terms
train_sq <- bind_cols(train1, sq_cols)
```

Checking linearity assumption
```{r}
# Test if any square terms are signifigant using LRT
brglm_sq <- brglm(INS ~., data = train_sq, 
                  family = binomial(link = "logit"))
anova(brglm_sq , brglm_fit, test = "LRT")  # yes they are sig at .001

# Stepwise selection
glm_sq <- glm(INS ~., data = train_sq, family = binomial(link = "logit"))
step_sq <- stepAIC(glm_sq, direction = c("both"))

# fit model suggested by the stepwise selection
train_sq_step <- train_sq %>% select(INS, names(step_sq$model)[-1])
brglm_step <- brglm(INS ~., data = train_sq_step, 
                    family = binomial(link = "logit"))
```

Checking for interactions
```{r}
# order coeficients by biggest to smallest
coef <- summary(brglm_step)
coef$coefficients %>% 
  as.data.frame() %>% 
  rownames_to_column("var") %>% 
  arrange(desc(Estimate))

# create interactions for the 3 predictors with biggests coef
train_int <- train_sq %>% 
  mutate(IAA_SB = INVT_ACCT_AMT * SAVBAL,
         IAA_DB = INVT_ACCT_AMT * DDABAL,
         SB_DB =  SAVBAL * DDABAL)

# fit model with the 3 interaction terms and test using LRT
brglm_int <- brglm(INS ~., data = train_int, 
                  family = binomial(link = "logit"))

anova(brglm_int, brglm_step , test = "LRT")   # yes they are sig at .001

# Stepwise selection
glm_int <- glm(INS ~., data = train_int, 
               family = binomial(link = "logit"))

step_int <- stepAIC(glm_int, direction = c("both"))

# fit model suggested by stepwise selection and main effects
train_int_step <- train_int %>% 
  select(names(step_int$model) )
brglm_step_int <- brglm(INS ~., data = train_int_step, 
                    family = binomial(link = "logit"))
```

Check for influential observations
```{r}
################### make this plot prettier ###########
# not influentital and removing obs could change the applicability domain of model

plot(brglm_step_int, 4)

cooksd <- data.frame(cooks.distance(brglm_step_int)) %>% 
  rownames_to_column("obs") %>%
  mutate(cooksd = round(cooks.distance.brglm_step_int.,6)) %>%
  select(-cooks.distance.brglm_step_int.) %>% 
  arrange(desc(cooksd))

cooksd %>% filter(obs == 6150)

train_int_step[6150,]
summary(brglm_step_int)
dfbetaPlots(brglm_step_int, terms = "SAVBAL_sq", id.n = 5)
# its the predictor savbal_sq that makes him influential. doesn't match
# model but he is strongly a yes based on other predictors
# removing that predictor makes his cooksd 0

brglm_SB <- brglm(INS ~., data = train_int_step %>% select(-SAVBAL_sq), 
                    family = binomial(link = "logit"))

data.frame(cooks.distance(brglm_SB)) %>% 
  rownames_to_column("obs") %>%
  mutate(cooksd = round(cooks.distance.brglm_SB.,6)) %>%
  select(-cooks.distance.brglm_SB.) %>% 
  arrange(desc(cooksd)) %>% filter(obs == 6150)
```

CIs for parameter estimates plot
```{r}
ci <- confint(brglm_step_int)

ci_df <- ci %>% 
  as.data.frame() %>% 
  rownames_to_column("Predictor") %>% 
  slice(-1) %>%
  mutate(rn = row_number(),
         Type = ifelse(rn <=6, "Main", "foo"),
         Type = ifelse(rn >6 & rn < 12, "Squared", Type),
         Type = ifelse(rn > 11, "Interaction", Type)) %>%
  select(-rn)

names(ci_df) <- c("Predictor", "low", "high", "Type")

ci_high <- ci_df %>% select(Predictor, high, Type) %>%
  rename("est" = high)

ci_low <- ci_df %>% select(Predictor, low, Type) %>%
  rename("est" = low)

ci_long <- bind_rows(ci_low, ci_high)

ggplot(data = ci_long, aes(x = est, y = Predictor, color = Type)) +
  geom_point() +
  geom_line() +
  geom_vline(xintercept = 0, linetype= "dotted", 
             color = "red", size=.9) +
  labs(title = "95% Confidence Intervals", 
       y = "Predictor", 
       x = "Parameter Estimate") +
  scale_x_continuous(breaks = seq(-1,2, by =.25)) +
  theme_bw() +
  theme(plot.title = element_text(hjust = .5, size = 22, face = "bold"),
        axis.text=element_text(size=13),
        axis.title=element_text(size=18),
        axis.line = element_line(colour = "black"),
        legend.text=element_text(size=13),
        legend.position=c(0.85, 0.35),legend.title=element_text(size=13)) 
```


Youden J
```{r}
pred <- prediction(fitted(brglm_step_int), train_int_step$INS)

perf_roc <- performance(pred, measure = "tpr", x.measure = "fpr")
roc <- performance(pred, measure = "auc")@y.values
roc

roc_table <- data.frame(threshold = perf_roc@alpha.values[[1]],
                        tpr = perf_roc@y.values[[1]],
                        tnr = 1 - perf_roc@x.values[[1]]) %>% 
  mutate(youdenJ = tpr + tnr -1)

# max youden j                            
roc_table %>% arrange(desc(youdenJ))

# confustion matrix based on youden j prob cutoff
probs_train <- data.frame(p = predict(brglm_step_int, type = "response")) %>%
  mutate(resp = ifelse(p > .2539469, "Yes", "No")) %>%
  mutate_at("resp", funs(as.factor))

confusionMatrix(data = probs_train$resp, 
                reference = train_int_step$INS,
                positive = "Yes")


```

Precision Recall
```{r}
perf_pr <- performance(pred, measure = "rec", x.measure = "prec")

pr_table <- data.frame(threshold = perf_pr@alpha.values[[1]],
                       Recall = perf_pr@y.values[[1]],
                       Precision = perf_pr@x.values[[1]])

# select the cutoff that has precision of at least .7 and the highest recall
cut <- pr_table %>% 
  filter(Precision > .7) %>% 
  arrange(desc(Recall)) %>% 
  slice(1) %>% 
  pull(threshold)                         
```


PR plot
```{r}
pr_long <- pr_table %>% 
  melt(id = "threshold") %>%
  rename("Metric"= variable) %>%
  filter(threshold < .70 & threshold > .40)

ggplot(pr_long, aes(x = threshold, y = value, color = Metric)) +
  geom_line() +
  labs(title = "Alternative Probability Cutoff",
       x = "Probability Cutoff", 
       y = "Metric Value") +
  theme_bw() +
  theme(plot.title = element_text(hjust = .5, size = 17, face = "bold"),
        axis.text=element_text(size=12),
        axis.title=element_text(size=15),
        axis.line = element_line(colour = "black"),
        legend.text=element_text(size=12),
        legend.position=c(0.85, 0.35),legend.title=element_text(size=13)) + 
  scale_y_continuous(breaks = seq(.1,.8, by =.1)) +
  scale_x_continuous(breaks = seq(.4,.7, by =.05)) +
  geom_vline(xintercept = cut, linetype="dotted", 
                color = "orange1", size=.8)
```





####3################# test set prep #######################

```{r}
test <- read_sas(paste0(raw,"insurance_v.sas7bdat"))

test <- test %>%  
  mutate(m1537 = ifelse(is.na(INCOME) & 
                          is.na(LORES) &
                          is.na(HMVAL), "Yes", "No"),
         m1075 = ifelse(is.na(PHONE) & 
                          is.na(POS) & 
                          is.na(POSAMT) & 
                          is.na(INV) & 
                          is.na(INVBAL) & 
                          is.na(CC) & 
                          is.na(CCBAL) & 
                          is.na(CCPURC), "Yes", "No"))

test <- test %>% 
  mutate_at(binary_cols, funs(str_replace_all(., c("1" = "Yes", "0" = "No")))) %>%
  mutate_at(cat_cols, funs(as.factor(.)))


```

Impute missing values and standarize data
```{r}
test_proc <- predict(proc, test)
```

Create new variables
```{r}
test_var <- test_proc %>% 
  mutate_at(c("CD", "IRA", "MM", "LOC", "ILS", "MTG"), funs(as.numeric(.))) %>%
  mutate(INVT_ACCT_AMT = CD + IRA + MM -3,
         INVT_ACCT_BAL = CDBAL + IRABAL + MMBAL,
         LN_ACCT_AMT = LOC + ILS + MTG - 3,
         LN_ACCT_BAL = LOCBAL + ILSBAL + MTGBAL) %>%
  select(-CD, -IRA, -MM, -CDBAL, -IRABAL, -MMBAL, -LOC,
         -ILS, -MTG, -LOCBAL, -ILSBAL, -MTGBAL)

test_var <- test_var %>%
  mutate(LOW_VOL_BRANCH = ifelse(BRANCH %in% bad_branch, "Yes", "No")) %>% 
  mutate_at('LOW_VOL_BRANCH', funs(as.factor(.))) %>%
  select(-BRANCH) 
```

Create square and interaction terms
```{r}
num_cols <- summarize_df(as.data.frame(test_var)) %>% 
  filter(type == "numeric") %>% 
  pull(col_name)

sq_cols <- test_var[, num_cols]

for (col in names(sq_cols)) {
  sq_cols[,col] <- sq_cols[,col] * sq_cols[,col]
}

sq_names <- names(sq_cols) %>% paste0("_sq")

names(sq_cols) <- sq_names

test_sq <- bind_cols(test_var, sq_cols)

test_int <- test_sq %>% 
  mutate(IAA_SB = INVT_ACCT_AMT * SAVBAL,
         IAA_DB = INVT_ACCT_AMT * DDABAL,
         SB_DB =  SAVBAL * DDABAL)
```

########## Perfromance Metrics #################

```{r}
mod <- brglm_step_int
df_t <- test_int
p <- predict(brglm_step_int, newdata = df_t, type = "response") %>% 
  unname()
y <- df_t %>% 
  select(INS) %>% 
  mutate_all(funs(as.character(.))) %>% 
  mutate_all(funs(str_remove_all(.,c("Yes" = '1', "No" = "0")))) %>%
  mutate_all(funs(as.numeric(.))) %>%
  pull(INS)

# PR AUC
pr_auc <- PRAUC(y_true = y, y_pred = p)

# coefficient of discrimination
D <- mean(fitted(mod)[mod$y == 1]) - mean(fitted(mod)[mod$y == 0])



# brier score
brier_score <- MSE(p, y)

# c-statistic
auroc <- AUC(y_pred = p, y_true = y)

```
ROC Curve
```{r}
pred_test <- prediction(p, factor(y))

perf_test <- performance(pred_test, measure = "tpr", x.measure = "fpr")

plot(perf_test, colorize = TRUE, main = "ROC Curve")
abline(a = 0, b = 1, lty = 2)
```

```{r}
p_df <- data.frame(p = p) %>% mutate(p_class = ifelse(p > cut, "Yes" , "No"))

cm <- confusionMatrix(data = factor(p_df$p_class), 
                      reference = df_t$INS, positive = "Yes")

draw_confusion_matrix(cm)
```





















































